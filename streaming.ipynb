{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "Chien-Lan Hsueh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "Load in needed modules, set up evironment to run PySpark and create a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules used in this assignment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "# set up to use Spark\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import window, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Streaming Data **Without** Overlapping Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start an input stream using the `Rate` format and set up the query for the aggregated sum with the following watermark, window and trigger settings:\n",
    "- watermark: 5 seconds \n",
    "- windows: 30 seconds long **with no overlap**\n",
    "- triggered: every 20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an input stream with rate of 1 row per second\n",
    "# execute a query for the aggregated sum:\n",
    "spdf = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 1) \\\n",
    "    .load()\n",
    "\n",
    "# set watermarking and 30-second windows without overlap\n",
    "myquery = spdf \\\n",
    "    .withWatermark(\"timestamp\", \"5 seconds\") \\\n",
    "    .groupBy(window(spdf.timestamp, \"30 seconds\")) \\\n",
    "    .sum() \\\n",
    "    .writeStream.outputMode(\"update\").format(\"memory\") \\\n",
    "    .trigger(processingTime = \"20 seconds\") \\\n",
    "    .queryName(\"task1\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the results by sending them to the console or save them into a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|              window|sum(value)|\n",
      "+--------------------+----------+\n",
      "|{2022-04-11 18:43...|       465|\n",
      "|{2022-04-11 18:43...|      1365|\n",
      "|{2022-04-11 18:44...|       123|\n",
      "|{2022-04-11 18:42...|         0|\n",
      "|{2022-04-11 18:44...|      3165|\n",
      "|{2022-04-11 18:44...|      2265|\n",
      "|{2022-04-11 18:45...|      1125|\n",
      "|{2022-04-11 18:46...|      4825|\n",
      "|{2022-04-11 18:45...|      4965|\n",
      "|{2022-04-11 18:45...|      4065|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the task 1 results on console\n",
    "spark.sql(\"SELECT * FROM task1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the task 1 results in json file in the folder `task1`\n",
    "# make sure there is no `task1` folder before running the following codes\n",
    "spark.sql(\"SELECT * FROM task1\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write.format(\"json\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .save(\"task1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the query\n",
    "myquery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Streaming Data **With** Overlapping Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start another query with an overlapping windows:\n",
    "- watermark: 5 seconds \n",
    "- windows: 30 seconds long **with 15 seconds overlap**\n",
    "- triggered: every 20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an input stream with rate of 1 row per second\n",
    "# execute a query for the aggregated sum:\n",
    "spdf = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 1) \\\n",
    "    .load()\n",
    "\n",
    "# use the same watermarking and 30-second windows but with a 15-second overlap\n",
    "myquery2 = spdf\\\n",
    "    .withWatermark(\"timestamp\", \"5 seconds\") \\\n",
    "    .groupBy(window(spdf.timestamp, \"30 seconds\", \"15 seconds\")) \\\n",
    "    .sum() \\\n",
    "    .writeStream.outputMode(\"update\").format(\"memory\") \\\n",
    "    .trigger(processingTime = \"20 seconds\") \\\n",
    "    .queryName(\"task2\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results on console and save into a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|              window|sum(value)|\n",
      "+--------------------+----------+\n",
      "|{2022-04-11 18:48...|      1665|\n",
      "|{2022-04-11 18:48...|       765|\n",
      "|{2022-04-11 18:48...|      1215|\n",
      "|{2022-04-11 18:49...|       623|\n",
      "|{2022-04-11 18:48...|       325|\n",
      "|{2022-04-11 18:49...|      2115|\n",
      "|{2022-04-11 18:47...|        55|\n",
      "|{2022-04-11 18:49...|      1793|\n",
      "|{2022-04-11 18:51...|       576|\n",
      "|{2022-04-11 18:51...|      3321|\n",
      "|{2022-04-11 18:49...|      3465|\n",
      "|{2022-04-11 18:49...|      3015|\n",
      "|{2022-04-11 18:50...|      5265|\n",
      "|{2022-04-11 18:50...|      3915|\n",
      "|{2022-04-11 18:49...|      2565|\n",
      "|{2022-04-11 18:50...|      4365|\n",
      "|{2022-04-11 18:50...|      4815|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the task 2 results on console\n",
    "spark.sql(\"SELECT * FROM task2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the task 2 results in json file in the folder `task2`\n",
    "# make sure there is no `task2` folder before running the following codes\n",
    "spark.sql(\"SELECT * FROM task2\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write.format(\"json\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .save(\"task2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the query\n",
    "myquery2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Streaming Accelerometer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepartion\n",
    "Before we start this task, we will do some preparation:\n",
    "1. Read in raw csv data\n",
    "2. Define helper functions (in execution order):\n",
    "    - 2.1 setup_stream(): Set up stream data and folders\n",
    "    - 2.2 start_query(): Create and start a query to read from input streams, perform calculation and write the output\n",
    "    - 2.3 start_stream(): Start streaming data   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepartion 1: Read in the raw csv file\n",
    "Read in the raw csv file, check it and list all unique values of pid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048573 entries, 0 to 1048572\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   time    1048573 non-null  float64\n",
      " 1   pid     1048573 non-null  object \n",
      " 2   x       1048573 non-null  float64\n",
      " 3   y       1048573 non-null  float64\n",
      " 4   z       1048573 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 40.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.493730e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>0.0758</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>-0.0102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.493730e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.0359</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.493730e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.2427</td>\n",
       "      <td>-0.0861</td>\n",
       "      <td>-0.0163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.493730e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.2888</td>\n",
       "      <td>0.0514</td>\n",
       "      <td>-0.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.493730e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.0413</td>\n",
       "      <td>-0.0184</td>\n",
       "      <td>-0.0105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           time     pid       x       y       z\n",
       "0  1.493730e+12  SA0297  0.0758  0.0273 -0.0102\n",
       "1  1.493730e+12  SA0297 -0.0359  0.0794  0.0037\n",
       "2  1.493730e+12  SA0297 -0.2427 -0.0861 -0.0163\n",
       "3  1.493730e+12  SA0297 -0.2888  0.0514 -0.0145\n",
       "4  1.493730e+12  SA0297 -0.0413 -0.0184 -0.0105"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values of pid: ['SA0297' 'PC6771' 'BK7610' 'DC6359' 'MC7070' 'MJ8002' 'BU4707' 'JR8022'\n",
      " 'HV0618' 'SF3079' 'JB3156' 'CC6740']\n"
     ]
    }
   ],
   "source": [
    "# specify working folder\n",
    "TaskDir = \"task3\"\n",
    "\n",
    "# raw data file\n",
    "CSVFile = \"all_accelerometer_data_pids_13.csv\"\n",
    "\n",
    "# read in the raw csv data\n",
    "df_raw = pd.read_csv(CSVFile)\n",
    "\n",
    "# look at the data\n",
    "display(df_raw.info())\n",
    "display(df_raw.head())\n",
    "\n",
    "# list of unique pid\n",
    "print(f\"The unique values of pid: {df_raw['pid'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepartion 2.1: Define a helper function to set up streams\n",
    "This function will subset raw data for each stream, create folders for streaming and checkpoints and return those paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up stream data and folders\n",
    "def setup_stream(df_raw, column, lst_stream, dir):\n",
    "    \"\"\"\n",
    "    Subset raw data for each stream, create folders for streaming and checkpoints and return those paths.\n",
    "    \n",
    "    df_raw: raw data in dataframe\n",
    "    column: column in the dataframe df_raw to specify types of data stream\n",
    "    lst_stream: lst of data streams \n",
    "    dir: working folder\n",
    "    \n",
    "    Return a zipped list of the following setup for each stream\n",
    "    dir_stream: list of stream folders for each stream\n",
    "    dir_checkpoint: list of checkpoint folders for each stream\n",
    "    lst_df: list of dataframes for each stream\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of data dumps (data streams)\n",
    "    n = len(lst_stream)\n",
    "    \n",
    "    # extract the data from the raw data for the selected pids\n",
    "    lst_df = [df_raw[df_raw[column] == _]  for _ in lst_stream]\n",
    "\n",
    "    # stream data folders for each stream: dir/<stream>/data\n",
    "    dir_stream = [os.path.join(dir, _, \"stream\") for _ in lst_stream]\n",
    "    dir_checkpoint = [os.path.join(dir, _, \"checkpoint\") for _ in lst_stream]\n",
    "\n",
    "    # create folders\n",
    "    ## (1) working folder\n",
    "    ## (2) in working folder, one data folder for each stream source\n",
    "    for x in [dir] + dir_stream + dir_checkpoint:\n",
    "        # create an empty data folder\n",
    "        try:\n",
    "            # if not exists, create one\n",
    "            os.makedirs(x)\n",
    "        except:\n",
    "            # if exists, delete and then create one\n",
    "            shutil.rmtree(x)\n",
    "            os.makedirs(x)   \n",
    "\n",
    "    # check if the setup is correct: stream data folders and data\n",
    "    [display(f\"{lst_stream[_]} stream data in folder '{dir_stream[_]}' (checkpoint: {dir_checkpoint[_]})\", lst_df[_]) for _ in range(n)]\n",
    "        \n",
    "    return list(zip(dir_stream, dir_checkpoint, lst_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepartion 2.2: Define a helper function to start a spark strucctured streaming query\n",
    "This function starts a query to \n",
    "- read a streaming data from a specified data folder (location)\n",
    "- cast the `x`, `y` and `z` variables to `double` numeric type\n",
    "- calculate the magnitude of acceleration data\n",
    "- write the output via a writestream with `append` output mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start a query to read from input streams, perform calculation and write the output\n",
    "def start_query(spark, stream_setup, dir):\n",
    "    \"\"\"\n",
    "    Start a query to read streaming data, calculate the magnitude of acceleration data and write the output.\n",
    "    \n",
    "    spark: a spark session\n",
    "    stream_setup: a list of the following setup for each stream\n",
    "        dir_stream: list of stream folders for each stream\n",
    "        dir_checkpoint: list of checkpoint folders for each stream\n",
    "        lst_df: list of dataframes for each stream     \n",
    "    dir: working folder\n",
    "    \"\"\"    \n",
    "    \n",
    "    # unpack the list of stream setup\n",
    "    dir_stream, dir_checkpoint, _ = stream_setup\n",
    "    \n",
    "    print(f\"Set up and start a query for the streaming data from the folder: '{dir_stream}'.\", end =\"\")\n",
    "    \n",
    "    # define schema for streaming data to read in\n",
    "    userSchema = StructType() \\\n",
    "        .add(\"time\", \"string\").add(\"pid\", \"string\") \\\n",
    "        .add(\"x\", \"string\").add(\"y\", \"string\").add(\"z\", \"string\")\n",
    "    \n",
    "    # read in stream and convert data types\n",
    "    # calculate mag and select columns for output\n",
    "    # set up output sink and start the query\n",
    "    query = spark \\\n",
    "        .readStream \\\n",
    "        .option(\"sep\", \",\") \\\n",
    "        .schema(userSchema) \\\n",
    "        .csv(dir_stream) \\\n",
    "        .select(col('time').cast(\"timestamp\"), 'pid', col('x').cast(\"double\"), col('y').cast(\"double\"), col('z').cast(\"double\")) \\\n",
    "        .withColumn('mag', (col('x')**2 + col('y')**2 + col('z')**2)**0.5 ) \\\n",
    "        .select(col('time'), col('pid'), col('mag')) \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"path\", dir) \\\n",
    "        .option(\"checkpointlocation\", dir_checkpoint) \\\n",
    "        .start()\n",
    "    \n",
    "    # Make sure we have enough time to get the query up and ready to wait for streaming data\n",
    "    # (Needs improvement - not a good way to do this, but good enough for now)\n",
    "    status = query.status\n",
    "    while status['message'] != \"Waiting for data to arrive\":\n",
    "        time.sleep(1)        \n",
    "        print(\".\", end =\"\")      \n",
    "        status = query.status\n",
    "    \n",
    "    print()\n",
    "    print(query.status)\n",
    "    print()\n",
    "    \n",
    "    # return the query for further operations\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepartion 2.3: Define a helper function to start streams\n",
    "This function starts generating csv files as streaming data into the specified folders for each stream, The default batch size is 500 rows of data with 10 seconds delay between batches of the streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming data\n",
    "def start_stream(stream_setup, batch_size = 500, batches = None, delay = 10):\n",
    "    \"\"\"\n",
    "    For each stream, start generating csv files into the stream folders\n",
    "    \n",
    "    stream_setup: a list of the following setup for each stream\n",
    "        dir_stream: list of stream folders for each stream\n",
    "        dir_checkpoint: list of checkpoint folders for each stream\n",
    "        lst_df: list of dataframes for each stream \n",
    "    batch_size: number of data rows to dump in each batch\n",
    "    batches: number of batches to dump. By default, use all of the available data rows\n",
    "    delay: delay time (in seconds) befor dumping next batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # unpack the list of stream setup\n",
    "    dir_stream, dir_checkpoint, lst_df = list(zip(*stream_setup))\n",
    "    \n",
    "    # number of data streams\n",
    "    n = len(stream_setup)\n",
    "    \n",
    "    # decide the maximum number of batches without running out of data\n",
    "    batches_max = min([_.shape[0] // batch_size for _ in lst_df])\n",
    "    \n",
    "    # decide number of batches to stream\n",
    "    batches = batches_max if batches is None else min(batches, batches_max)\n",
    "    print(f\"{batches} batches to stream:\")\n",
    "\n",
    "    # for each pid, generate streaming data and save as csv files in their own stream data folder\n",
    "    for i in range(batches):\n",
    "        print(i+1, end = \" \")\n",
    "        for _ in range(n):\n",
    "            # get the next batch of rows        \n",
    "            temp = lst_df[_][i*batch_size : (i+1)*batch_size].copy()\n",
    "            # write to csv file\n",
    "            temp.to_csv(os.path.join(dir_stream[_], str(i+1) + \".csv\"), index = False, header = False)\n",
    "            \n",
    "        # delay before sending next batch\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    print(\"Done!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up steaming and start the queries\n",
    "With all the preparation done, we are ready for task 3. Here are the steps:\n",
    "1. Set up stream data and folders for each selected pid\n",
    "2. Start 2 querries to monitor the stream folders. If any data come in, do the calculation and output the results\n",
    "3. Check the query status\n",
    "4. Start the data streaming to send in data\n",
    "5. Output teh results to a single csv file\n",
    "6. Stop the queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "Set up two streaming data for the personr with pid 'SA0297', 'PC6771':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SA0297 stream data in folder 'task3\\\\SA0297\\\\stream' (checkpoint: task3\\\\SA0297\\\\checkpoint)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.493730e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>0.0758</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>-0.0102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.493730e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.0359</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.493730e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.2427</td>\n",
       "      <td>-0.0861</td>\n",
       "      <td>-0.0163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.493730e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.2888</td>\n",
       "      <td>0.0514</td>\n",
       "      <td>-0.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.493730e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.0413</td>\n",
       "      <td>-0.0184</td>\n",
       "      <td>-0.0105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31040</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.0186</td>\n",
       "      <td>-0.0029</td>\n",
       "      <td>-0.0105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31041</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>-0.0246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31042</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31043</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-0.0062</td>\n",
       "      <td>-0.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31044</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.0018</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>-0.0040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28048 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               time     pid       x       y       z\n",
       "0      1.493730e+12  SA0297  0.0758  0.0273 -0.0102\n",
       "1      1.493730e+12  SA0297 -0.0359  0.0794  0.0037\n",
       "2      1.493730e+12  SA0297 -0.2427 -0.0861 -0.0163\n",
       "3      1.493730e+12  SA0297 -0.2888  0.0514 -0.0145\n",
       "4      1.493730e+12  SA0297 -0.0413 -0.0184 -0.0105\n",
       "...             ...     ...     ...     ...     ...\n",
       "31040  1.493740e+12  SA0297 -0.0186 -0.0029 -0.0105\n",
       "31041  1.493740e+12  SA0297  0.0095 -0.0026 -0.0246\n",
       "31042  1.493740e+12  SA0297  0.0050  0.0071  0.0024\n",
       "31043  1.493740e+12  SA0297  0.0005 -0.0062 -0.0111\n",
       "31044  1.493740e+12  SA0297 -0.0018 -0.0003 -0.0040\n",
       "\n",
       "[28048 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"PC6771 stream data in folder 'task3\\\\PC6771\\\\stream' (checkpoint: task3\\\\PC6771\\\\checkpoint)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27927</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>PC6771</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>-0.0113</td>\n",
       "      <td>0.0051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27928</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>PC6771</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>-0.0088</td>\n",
       "      <td>0.0171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27929</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>PC6771</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>-0.0088</td>\n",
       "      <td>0.0172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27930</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>PC6771</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>0.0072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27931</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>PC6771</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>0.0135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737144</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>PC6771</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737150</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>PC6771</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737157</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>PC6771</td>\n",
       "      <td>-0.0052</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737164</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>PC6771</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737170</th>\n",
       "      <td>1.493740e+12</td>\n",
       "      <td>PC6771</td>\n",
       "      <td>-0.0044</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207753 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                time     pid       x       y       z\n",
       "27927   1.493740e+12  PC6771  0.0040 -0.0113  0.0051\n",
       "27928   1.493740e+12  PC6771  0.0039 -0.0088  0.0171\n",
       "27929   1.493740e+12  PC6771 -0.0012 -0.0088  0.0172\n",
       "27930   1.493740e+12  PC6771  0.0033 -0.0048  0.0072\n",
       "27931   1.493740e+12  PC6771  0.0051 -0.0033  0.0135\n",
       "...              ...     ...     ...     ...     ...\n",
       "737144  1.493740e+12  PC6771 -0.0008 -0.0004  0.0009\n",
       "737150  1.493740e+12  PC6771 -0.0012  0.0013  0.0018\n",
       "737157  1.493740e+12  PC6771 -0.0052  0.0007  0.0089\n",
       "737164  1.493740e+12  PC6771  0.0000  0.0021  0.0029\n",
       "737170  1.493740e+12  PC6771 -0.0044  0.0004  0.0104\n",
       "\n",
       "[207753 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are totally 2 streams being set up.\n"
     ]
    }
   ],
   "source": [
    "# pid of interest\n",
    "lst_pid = ['SA0297', 'PC6771']\n",
    "\n",
    "# set up streams for each pid\n",
    "stream_setup = setup_stream(df_raw, column = \"pid\", lst_stream = lst_pid, dir = TaskDir)\n",
    "print(f\"There are totally {len(stream_setup)} streams being set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "Next we will start 2 queries, one for each data stream, to calulcate the magnitude of acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up and start a query for the streaming data from the folder: 'task3\\SA0297\\stream'...\n",
      "{'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "\n",
      "Set up and start a query for the streaming data from the folder: 'task3\\PC6771\\stream'..\n",
      "{'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "\n",
      "There are totally 2 queries started.\n"
     ]
    }
   ],
   "source": [
    "# get queries for each pid and start them\n",
    "queries = [start_query(spark, _, dir = TaskDir) for _ in stream_setup]\n",
    "print(f\"There are totally {len(queries)} queries started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "Check the stauts of the queries to confirm they are started and active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "{'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x1f1a2928490>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x1f1a292b010>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report status of the queries\n",
    "[print(_.status) for _ in queries]\n",
    "\n",
    "# list active streams\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "Once the queries are up running and ready for data, we can start the data streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 batches to stream:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 Done!\n"
     ]
    }
   ],
   "source": [
    "# here we use all data available (maximum number of batches)\n",
    "# in case a quick test is prefered: \n",
    "# `start_stream(stream_setup, batch_size = 500, batches = 20, delay = 10)`\n",
    "start_stream(stream_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5\n",
    "Get one single csv file for the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PySpark to read in all \"part\" files\n",
    "allfiles = spark.read.option(\"header\",\"false\").csv(os.path.join(TaskDir, \"part-*.csv\"))\n",
    "\n",
    "# set up folder for the single csv file\n",
    "dir_single_csv = os.path.join(TaskDir, \"single_csv_file\")\n",
    "try:\n",
    "    # if exists, delete it\n",
    "    shutil.rmtree(dir_single_csv)\n",
    "except:\n",
    "    # otherwise do nothing\n",
    "    pass\n",
    "\n",
    "# Output as CSV file\n",
    "allfiles \\\n",
    "    .coalesce(1) \\\n",
    "    .write.format(\"csv\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .save(dir_single_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6\n",
    "Stop the queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop all queries\n",
    "[_.stop() for _ in queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries with More Streams\n",
    "The following is a workflow for another test with more streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify working folder\n",
    "TaskDir = \"task3-2nd\"\n",
    "\n",
    "# pid of interest\n",
    "lst_pid = ['BK7610', 'DC6359', 'MC7070', 'MJ8002', 'BU4707']\n",
    "\n",
    "# set up streams for each pid\n",
    "stream_setup = setup_stream(df_raw, column = \"pid\", lst_stream = lst_pid, dir = TaskDir)\n",
    "print(f\"There are totally {len(stream_setup)} streams being set up.\")\n",
    "\n",
    "# report status of the queries\n",
    "[print(_.status) for _ in queries]\n",
    "\n",
    "# list active streams\n",
    "spark.streams.active\n",
    "\n",
    "# a quicker and shorter test:\n",
    "start_stream(stream_setup, batch_size = 500, batches = 20, delay = 5)\n",
    "\n",
    "# Use PySpark to read in all \"part\" files\n",
    "allfiles = spark.read.option(\"header\",\"false\").csv(os.path.join(TaskDir, \"part-*.csv\"))\n",
    "\n",
    "# set up folder for the single csv file\n",
    "dir_single_csv = os.path.join(TaskDir, \"single_csv_file\")\n",
    "try:\n",
    "    # if exists, delete it\n",
    "    shutil.rmtree(dir_single_csv)\n",
    "except:\n",
    "    # otherwise do nothing\n",
    "    pass\n",
    "\n",
    "# Output as CSV file\n",
    "allfiles \\\n",
    "    .coalesce(1) \\\n",
    "    .write.format(\"csv\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .save(dir_single_csv)\n",
    "\n",
    "# stop all queries\n",
    "[_.stop() for _ in queries]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "662f86ef376671a988e108b9a80f41ae296e240171aee519bbf45f59b71f9158"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
